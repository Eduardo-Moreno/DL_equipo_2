![](https://github.com/optimizacion-2-2021-1-gh-classroom/practica-1-segunda-parte-caroacostatovany/blob/main/src/docs/images/mex_simplex_logo.png)
# Similitud Semántica::BERT
***

### Objetivo

El objetivo del presente trabajo, es estudiar el modelo **BERT** (*Bidirectional Encoder Representatios from Transformers*), el cual es un modelo bidireccional para el análisis de lenguaje natural que busca encontrar la similitud entre 2 oraciones, en cuanto a su significado.

### Descripción del repo:

- Utilizamos Google Colab (GRU) para el estudio del modelo del desempeño del modelo: `notebooks/`.
- El reporte, código y resumen técnico se encuentra en la carpeta `documentation/`
- En la carpeta `models/` se encuentran los modelos que evaluaron el desempeño de BERT bajo distintas configuraciones de los hiperparámetros.


## Equipo 2

| Nombre | User Github | 
|:---:|:---:|
| Hécttor Efrén Guadarrama | HecttorJ | 
| Eduardo Moreno | Eduardo-Moreno| 
| Miguel López Cruz | Lobolc| 
